{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mengenal TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada sesi pemeblejaran kali ini, kita akan mempelajari salah satu teknik yang umum ditemui untuk melakukan feature extraction pada sekumpulan teks atau dokumen yaitu \"Term Frequency - Inverse Document Frequency\" atau biasa disingkat sebagai TF-IDF. Pada sesi pembelajaran sebelumnya, kita sudah mempelajari dua teknik dasar yang umum ditemui dalam text processing yaitu \"Back of Word\" dan \"Stop Word Filtering\".\n",
    "\n",
    "Dalam sesi pembelajaran kita kali ini, kita akan mempelajari suatu teknik untuk menghitung bobot suatu kata terhadap suatu dokumen dari sekumpulan dokumen. Teknik ini dikenal sebagai \"Term Frequency - Inverse Document Frequency\" atau biasa disingkat TF-IDF. Teknik ini juga cukup umum ditemui dalam bidang information retrieval.  \n",
    "\n",
    "TF-IDF (Term Frequency - Inverse Document Frequency) merupakan salah satu metode statistik yang digunakan untuk mengukur seberapa penting suatu kata terhadap suatu dokumen tertentu dari sekumpulan dokumen atau corpus. TF-IDF ini pada dasarnya melibatkan perkalian antara dua nilai yaitu nilai TF atau Term Frequency dengan nilai IDF atau Inverse Document Frequency.\n",
    "\n",
    "Referensi:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "- https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF (Term Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama-tama kita akan mempelajari TF atau Term Frequency terlebih dahulu. Terdapat beberapa opsi formula yang bisa digunakan untuk merepresentasikan Term Frequency. Forumula yang paling sederhana adalah dengan menghitung jumlah kemunculan suatu term atau suatu kata pada suatu dokumen, formula semacam ini sering kali dikenal sebagai row count dengan formula $f_{{t,d}}$, hanya saja implementasi Term Frequency pada SkLearn tidak mengadopsi formula semacam ini. Term Frequency yang diimplementasikan pada SkLearn menerapkan formula sebagai berikut :\n",
    "\n",
    "Term Frequency Adjusted for Document Length, dimana term frequency diekspreksikan sebagai hasil pembagian antara jumlah kemunculan suatu term pada document dengan total jumlah kata yang terkandung dalam document tersebut dengan formula matematis sebagai berikut :\n",
    "\n",
    "${\\displaystyle f_{t,d}{\\Bigg /}{\\sum _{t'\\in d}{f_{t',d}}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Document Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya kita akan mempelajari \"Inverse Document Frequency\". Sama halnya dengan \"Term Frequency\", disini juga terdapat beberapa opsi formula yang dapat digunakan untuk merepresentasikan Inverse Document Frequency. Formula yang paling sederhana adalah dengan menghitung nilai log dari pembagian antara total jumlah document dalam suatu corpus dengan jumlah document yang mengandung term tertentu. \n",
    "\n",
    "Berikut merupakan implementasinya secara matematis :\n",
    "\n",
    "${\\displaystyle \\log {\\frac {N}{n_{t}}}=-\\log {\\frac {n_{t}}{N}}}$\n",
    "\n",
    "Hanya saja implementasi inverse document frequency pada SkLearn memiliki formula sebagai berikut :\n",
    "\n",
    "idf$(t) = log \\frac{1+n}{1+df(t)} + 1$\n",
    "\n",
    "- $n$ : Merepresentasikan total jumlah dokumen dalam suatu corpus.\n",
    "- $df(t)$ : Merepresentasikan jumlah dokumen yang mengandung term tertentu\n",
    "\n",
    "Formula diatas merupakan formula untuk merepresentasikan inverse document frequency. Selain itu, SKLearn juga menerapkan l2 normalization atau pada kalkulasi pemboboton \"TF-IDF\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the house had a tiny little mouse',\n",
       " 'the cat saw the mouse',\n",
       " 'the mouse ran away from the house',\n",
       " 'the cat finally ate the mouse',\n",
       " 'the end of the mouse story']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 01\n",
    "\n",
    "corpus = [\n",
    "    'the house had a tiny little mouse', \n",
    "    'the cat saw the mouse',\n",
    "    'the mouse ran away from the house', \n",
    "    'the cat finally ate the mouse',\n",
    "    'the end of the mouse story'\n",
    "]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alur berpikir code 01 :\n",
    "- Pertama-tama kita akan menyiapkan terlebih dahulu suatu dataset text.\n",
    "- Untuk kasus kita kali ini, datasetnya berupa sekumpulan kalimat pendek. Seperti kita pelajari pada sesi sebelumnya, dataset ini juga seringkali dikenal dengan istilah \"corpus\".\n",
    "- Lalu kita akan coba tampilkan ke layar untuk corpusnya.\n",
    "\n",
    "Jika kita perhatikan pada code 01, corpus yang kita miliki pada kali ini terdiri dari lima buah kalimat pendek dengan kalimat sebagai berikut :\n",
    "- Kalimat pertama yaitu 'the house had a tiny little mouse'.\n",
    "- Kalimat kedua yaitu 'the cat saw the mouse'.\n",
    "- Kalimat ketiga yaitu 'the mouse ran away from the house'.\n",
    "- Kalimat keempat yaitu 'the cat finally ate the mouse'.\n",
    "- Kalimat kelima yaitu 'the end of the mouse story'.\n",
    "\n",
    "Kita akan menggunakan corpus tersebut sebagai studi kasuk kita pada kali ini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Weights dengan TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya kita akan menerapkan TF-IDF pada dataset corpus yang kita miliki. Pada SkLearn, TF-IDF dapat diterapkan dengan modul  TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.2808823162882302\n",
      "  (0, 6)\t0.5894630806320427\n",
      "  (0, 11)\t0.5894630806320427\n",
      "  (0, 5)\t0.47557510189256375\n",
      "  (1, 9)\t0.7297183669435993\n",
      "  (1, 2)\t0.5887321837696324\n",
      "  (1, 7)\t0.3477147117091919\n",
      "  (2, 1)\t0.5894630806320427\n",
      "  (2, 8)\t0.5894630806320427\n",
      "  (2, 7)\t0.2808823162882302\n",
      "  (2, 5)\t0.47557510189256375\n",
      "  (3, 0)\t0.5894630806320427\n",
      "  (3, 4)\t0.5894630806320427\n",
      "  (3, 2)\t0.47557510189256375\n",
      "  (3, 7)\t0.2808823162882302\n",
      "  (4, 10)\t0.6700917930430479\n",
      "  (4, 3)\t0.6700917930430479\n",
      "  (4, 7)\t0.3193023297639811\n"
     ]
    }
   ],
   "source": [
    "# Code 02\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "response = vectorizer.fit_transform(corpus)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alur berpikir code 02 :\n",
    "- Pertama-tama kita akan mengimport modulnya terlebih dahulu dengan memanggil \"from sklearn.feature_extraction.text import TfidfVectorizer\".\n",
    "- Lalu pada tahapan berikutnya, kita akan membentuk objek dari class TfidfVectorizer tersebut dengan menyertakan parameter (stop_words) yang kita beri nilai 'english'.\n",
    "- Selanjutnya objek TF-IDF tersebut kita akan tampung ke dalam variabel \"vectorizer\".\n",
    "- Pada tahapan berikutnya, kita akan memanggil method fit.transform dari objek \"vectorizer\" tersebut dan akan kita terapkan pada (corpus) yang kita miliki.\n",
    "- Lalu hasil fit.transform nya kita akan tampung kedalam variabel \"response\".\n",
    "- Untuk selanjutnya kita akan coba tampilkan ke layar dengan print.\n",
    "\n",
    "Berdasarkan hasil output 02 terdiri dari angka-angka yang sangat banyak. Kita akan mulai mencoba memahami maksud output tersebut. Pada hasil output sisi kiri, angka-angkanya merepresentasikan indeks dari corpus yang kita miliki yang terdiri dari angka 0, 1, 2, 3, dan 4.\n",
    "- Indeks 0 merepresentasikan indeks kalimat pertama dari corpus kita.\n",
    "- Indeks 1 merepresentasikan indeks kalimat kedua dari corpus kita.\n",
    "- Indeks 2 merepresentasikan indeks kalimat ketiga dari corpus kita.\n",
    "- Indeks 3 merepresentasikan indeks kalimat keempat dari corpus kita.\n",
    "- Indeks 4 merepresentasikan indeks kalimat kelima dari corpus kita.\n",
    "\n",
    "Lalu berikutnya angka pada kolom yang kedua. Angka-angka tersebut merepresentasikan indeks dari features name yang dihasilkan dari Bag of Words kita. Untuk lebih jelasnya, selanjutnya kita akan panggil \"vectorizer.get_feature_names()\" perhatikan pada code 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ate',\n",
       " 'away',\n",
       " 'cat',\n",
       " 'end',\n",
       " 'finally',\n",
       " 'house',\n",
       " 'little',\n",
       " 'mouse',\n",
       " 'ran',\n",
       " 'saw',\n",
       " 'story',\n",
       " 'tiny']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 03 \n",
    "\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan hasil output 03, kita bisa menyimpulkan beberapa hal sebagai berikut :\n",
    "\n",
    "- Berdasarkan hasil output code 03 merupakan kumpulan token yang sudah di eliminier stop wordnya. Bisa kita lihat ada 'ate', 'away', 'cat', 'end', 'finally', 'house', 'little', 'mouse', 'ran', 'saw', 'story', dan 'tiny'. \n",
    "- Token-token tersebut diurutkan berdasarkan urutan alphabetic. Jika kita perhatikan disini, untuk kalimat indeks ke 0, disni ada 7, 6, 11, dan 5 artinya kalimat pada indeks ke 0 tersebut, mengandung token indeks ke 7 berarti pada kalimat tersebut mengandung kata 'mouse', dan jika kita perhatikan pada dataset kita di awal pada corpus, pada kalimat pertama indeks ketujuh yaitu mengandung kata 'mouse'. \n",
    "- Selanjunya juga mengandung pada toke indeks ke 6, dan token pada indeks ke 6 adalah 'little'. Bisa kita lihat bahwa cocok karena kalimat pertama juga mengandung kata 'little'.\n",
    "- Lalu berikutnya juga mengandung token pada indeks ke 5, dan token pada indeks kelima adalah 'house'. Disini juga kita bisa lihat bahwa cocok karena pada kalimat pertama dari corpus juga mengandung kata 'house'.\n",
    "- Lalu berikutnya disni mengandung pada indeks ke 11. Token pada indeks ke 11 adalah 'tiny'. Bisa kita lihat juga cocok karena teradapat kata 'tiny' pada kalimat pertama dari corpus.\n",
    "\n",
    "Jadi bisa kita simpulkan bahwa baris paling kiri dari hasil code 02, merepresentasikan indek dari dokumen pada corpus kita, sedangkan pada baris tengah merepresentasikan indeks dari token yang terdapat dalam kalimat tersebut, dan pada baris ketiga yang berupa sekumpulan angka, merepresentasikan bobot dari TF-IDF hasil kalkulasi yang dilakukan oleh TF-IDF Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4755751 , 0.58946308, 0.28088232, 0.        , 0.        ,\n",
       "         0.        , 0.58946308],\n",
       "        [0.        , 0.        , 0.58873218, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.34771471, 0.        , 0.72971837,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.58946308, 0.        , 0.        , 0.        ,\n",
       "         0.4755751 , 0.        , 0.28088232, 0.58946308, 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.58946308, 0.        , 0.4755751 , 0.        , 0.58946308,\n",
       "         0.        , 0.        , 0.28088232, 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.67009179, 0.        ,\n",
       "         0.        , 0.        , 0.31930233, 0.        , 0.        ,\n",
       "         0.67009179, 0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 04\n",
    "\n",
    "response.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk lebih jelasnya pada code 04 ini kita akan mencoba mentransformasikan hasil response nya kedalam bentuk array dengan memanggil function \"response.todense()\".\n",
    "\n",
    "Pada hasil output code 04 disini akan terbentuk array dua dimensi, dimana setiap row nya tersebut akan merepresentasikan setiap kalimat atau setiap document pada corpus kita. Agar tampilannya lebih manis dan rapih, kita akan mecoba untuk mengkonversikan kedalam format Pandas DataFrame. \n",
    "\n",
    "Jika kita perhatikan pada code 04 kembali, bentuk array nya adalah array dua dimensi, dimana setiap barisnya ini merepresentasikan kalimatnya, hanya saja pada code 05 dalam bentuk pandas DataFrame, kita akan coba tampilkan kalimatnya dalam bentuk kolom, sedangkan barisnya akan digunakan untuk merepresentasikan setiap tokennya. Oleh karenanya, kita butuh untuk melakukan proses transpose terhadap array code 04 tersebut (lihat pada code 05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.589463</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>away</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.589463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cat</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.588732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475575</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>end</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.670092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>finally</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.589463</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>house</td>\n",
       "      <td>0.475575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>little</td>\n",
       "      <td>0.589463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mouse</td>\n",
       "      <td>0.280882</td>\n",
       "      <td>0.347715</td>\n",
       "      <td>0.280882</td>\n",
       "      <td>0.280882</td>\n",
       "      <td>0.319302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ran</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.589463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>saw</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.729718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>story</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.670092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tiny</td>\n",
       "      <td>0.589463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               D1        D2        D3        D4        D5\n",
       "ate      0.000000  0.000000  0.000000  0.589463  0.000000\n",
       "away     0.000000  0.000000  0.589463  0.000000  0.000000\n",
       "cat      0.000000  0.588732  0.000000  0.475575  0.000000\n",
       "end      0.000000  0.000000  0.000000  0.000000  0.670092\n",
       "finally  0.000000  0.000000  0.000000  0.589463  0.000000\n",
       "house    0.475575  0.000000  0.475575  0.000000  0.000000\n",
       "little   0.589463  0.000000  0.000000  0.000000  0.000000\n",
       "mouse    0.280882  0.347715  0.280882  0.280882  0.319302\n",
       "ran      0.000000  0.000000  0.589463  0.000000  0.000000\n",
       "saw      0.000000  0.729718  0.000000  0.000000  0.000000\n",
       "story    0.000000  0.000000  0.000000  0.000000  0.670092\n",
       "tiny     0.589463  0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 05\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(response.todense().T,\n",
    "                 index=vectorizer.get_feature_names(),\n",
    "                 columns=[f'D{i+1}' for i in range(len(corpus))])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alur berpikir code 05 :\n",
    "- Pertama-tama kita import terlebih dahulu modul pandasnya dengan memanggil \"import pandas as pd\".\n",
    "- Lalu kita panggil \"response.todense().T\". T pada code tersebut artinya adalah transpose.\n",
    "- Lalu kita panggil \"index=vectorizer.get_feature_names()\" untuk menampilkan nama-namanya yang dalam hal ini kita jadikan sebagai feature.\n",
    "- Selanjutnya kita akan panggil \"columns=[f'D{i+1}' for i in range(len(corpus))]\" untuk menampilkan semua baris dari corpusnya.\n",
    "- Lalu kita coba tampilkan ke layar.\n",
    "\n",
    "Pada hasil output code 05, disini informasinya menjadi lebih jelas, kita bisa melihat document 1 atau D1, D2, D3, D4, dan D5. Baris pertama merepresentasikan token 'ate' dan seterusnya sampai token 'tiny'. Cara membacanya yaitu untuk 'cat' kata cat terdapat dalam document ke-2 dan document ke-4, hanya saja kata cat ini memiliki bobot yang lebih tinggi pada document ke-2 bila dibandingkan pada document ke-4. Nilai pembobotan yang berada pada hasil output code 05 tersebut merupakan nilai pembobotan yang sudah di normaliasikan dengan menggunakan L2 Normalization yang nilai terkecilnya adalah 0,0 dan nilai terbesarnya adalah 1,0. Semakin tinggi bobot suatu kata pada suatu document, mengindikasikan bahwa kata tersebut semakin layak untuk digunakan sebagai keyword atau kata pencarian terhadap dokumen tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk pembelajaran lebih lengkapnya, jangan lupa kunjungi channel youtube Indonesia Belajar pada link berikut : https://www.youtube.com/watch?v=f0a1XXmaQp8 dan jangan lupa untuk like, comment, dan share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Banyak Belajar, Biar Bisa Bantu Banyak Orang\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by : Clarence Code Pianist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
