{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mengenal Text Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada sesi pembelajaran kita kali ini, kita akan mempelajari 2 macam text processing yang umum ditemui dalam bidang machine learning yaitu :\n",
    "1. Bag of Words.\n",
    "2. Stop Word Filtering.\n",
    "\n",
    "Dari pembelajaran sebelumnya, kita sudah megetahui bahwa komputer atau machine tidak bisa memahami data text dengan baik. Lalu bagiamana ketika kita dihadapkan pada sekumpulan data text dalam bentuk kalimat atau dokumen? Dalam machine learning, terdapat bidang yang secara spesifik membahas dataset text, bidang ini dikenal sebagai \"Natural Language Processing\" atau yang biasanya disingkat sebagai NLP. Yang kita pelajari pada sesi kali ini bisa kita manfaatkan sebagai pengantar NLP.\n",
    "\n",
    "Dalam bidang machine learning sendiri terdapat beberapa teknik yang umum digunakan untuk melakukan features extraction dari dataset text. Pada sesi pemebelajaran kali ini, kita akan mempelajari dua diantaranya, yaitu \"Bag of Words\" dan \"Stop Word Filtering\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model sebagai Representasi Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words menyederhanakan representasi text sebagai sekumpulan kata serta mengabaikan grammar dan posisi tiap kata pada kalimat. Text akan dikonversi menjadi lowcase dan tanda baca akan diabaikan.\n",
    "\n",
    "Referensi : http://en.wikipedia.org/wiki/Bag-of-words_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the linux kernel.',\n",
       " 'Linux is one of the most prominent open-source software.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 01\n",
    "\n",
    "corpus = [\n",
    "    'Linux has been around since the mid-1990s.',\n",
    "    'Linux distributions include the linux kernel.',\n",
    "    'Linux is one of the most prominent open-source software.'\n",
    "]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alur berpikir code 01 :\n",
    "\n",
    "- Pertama-tama kita akan siapkan terlebih dahulu suatu data text. Untuk kasus kita kali ini, dataset nya berupa sekumpulan kalimat pendek. Dataset text ini juga seringkali dikenal dengan istilah \"Corpus\".\n",
    "- Untuk kasus kita kali ini, corpus kita terdiri dari tiga kalimat pendek yaitu : kalimat pertama adalah 'Linux has been around since the mid-1990s.'. Kalimat kedua 'Linux distribuutions include the linux kernel.', dan kalimat ketiga adalah 'Linux is one of the most prominent open-source software.'.\n",
    "- Ketiga kalimat tersebut akan kita tampung kedalam suatu list yang kemudian kita assign kedalam suatu variabel yang kita beri nama 'corpus'.\n",
    "- Lalu terakhir kita akan coba tampilkan isi dari variabel 'corpus' tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model dengan CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya kita akan memanfaatkan \"Bag of Words\" untuk melakukan features extraction dari dataset yang kita miliki. Pada SkLearn, \"Bag of Words\" model dapat diterapkan dengan memanfatkan \"Count Vectorizer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]],\n",
       "       dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 02\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alur berpikir code 02 :\n",
    "- Pertama-tama kita akan akan mengimport terlebih dahulu modulnya dengan memanggil \"from sklearn.feature_extraction.text import CountVectorizer\".\n",
    "- Lalu selanjutnya kita akan membentuk objek dari kelas \"CountVectorizer\" dengan memanggil \"CountVectorizer()\" yang akan ditampung kedalam variabel \"vectorizer\".\n",
    "- Selanjutnya kita akan menggunakan objek \"vectorizer\" ini untuk menerapkan method fit transform terhadap \"corpus\" dataset lalu hasil yang terbentuk akan kita konversikan kedalam suatu array. Oleh karenanya, disini kita memanggil method \"todense()\" atau dengan kata lain method todense ini akan mengkonversikan hasil fit transform dari objek \"vectorizer\" tersebut menjadi suatu array dua dimensi yang diamana objek array dua dimensi terebut akan ditampung kedalam variabel \"vectorized_X\".\n",
    "- Terakhir kita akan coba tampilkan ke layar.\n",
    "\n",
    "Bisa kita lihat pada output code 02, hasilnya berupa array 2 dimensi yang setiap barisnya akan merepresentasikan tiap kalimat yang berada dalam corpus yang kita miliki. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'around',\n",
       " 'been',\n",
       " 'distributions',\n",
       " 'has',\n",
       " 'include',\n",
       " 'is',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'most',\n",
       " 'of',\n",
       " 'one',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'since',\n",
       " 'software',\n",
       " 'source',\n",
       " 'the']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 03\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alur berpikir code 03 :\n",
    "\n",
    "- Ketika kita memanggil method \"get_feature_names\" maka akan dikembalikan sekumpulan kata yang berada dalam bag atau keranjang atau lebih jelasnya yaitu sekumpulan kata dalam keranjang atau dalam bag. Jika kita perhatikan pada hasil code 03.\n",
    "- kumpulan kata tersebut sudah tidak lagi diurutkan berdasarkan urutan kalimat, melainkan sudah diurutkan berdasarkan alphabetical order. \n",
    "- Jika kita perhatikan lagi pada hasil output 03, semua casenya menjadi lower case artinya  sudah tidak ada lagi huruf besar atau uppercasenya.\n",
    "- Setiap kata yang ditamung dalam bag atau keranjang tersebut, juga dikenal dengan istilah \"token\".\n",
    "- Nilai pada array tersebut tidak hanya bernilai nol dan satu. Setiap nilai tersebut merepresentasikan jumlah kemunculan token/kata tertentu pada kalimat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclidean Distance untuk Mengukur Kedekatan/Jarak Antar Dokumen (Vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada kali ini kita akan mempelajari maksud dari representasi dari \"Bag of Word\". Dengan representasi dari \"Bag of Word\", suatu algoritma machine learning dapat dengan lebih mudah mengukur kedekatan atau kemiripan antar dokumen.\n",
    "\n",
    "Untuk mengukur kedekatannya, kita akan menggunakan \"Euclidean Distance\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarak dokumen 1 dan 2: [[3.16227766]]\n",
      "Jarak dokumen 1 dan 3: [[3.74165739]]\n",
      "Jarak dokumen 2 dan 3: [[3.46410162]]\n"
     ]
    }
   ],
   "source": [
    "# Code 04\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "for i in range(len(vectorized_X)):\n",
    "    for j in range(i, len(vectorized_X)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        jarak = euclidean_distances(vectorized_X[i], vectorized_X[j])\n",
    "        print(f'Jarak dokumen {i+1} dan {j+1}: {jarak}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alur berpikir code 04 :\n",
    "- Pertama-tama kita akan mengimport terlebih dahulu modulnya dengan memanggil \"from sklearn.metrics.pairwise import euclidean_distances\".\n",
    "- Lalu berikutnya, kita akan coba untuk mengukur jarak antar kalimatnya berarti kalimat pertama akan diukur kedekatannya dengan kalimat kedua, lalu kalimat pertama juga akan diukur kedekatan atau jaraknya dengan kalimat ketiga. Selanjutnya, kita juga akan mengukur kedekatan atau jarak antara kalimat kedua dengan kalimat ketiganya.\n",
    "- Proses tersebut kita akan lakukan dengan memanfaatkan for loop statement (lihat pada code 04).\n",
    "\n",
    "Bisa kita lihat berdasarkan hasil output 04, jarak antara dokumen pertama dan kedau adalah 3.16227766, lalu jarak antara dokumen pertama dengan dokumen ketiga adalah 3.74165739, dan jarak antara dokumen keduan dengan dokumen ketiga adalah 3.46410162. Jika kita perhatikan disini, nilai terkecilnya adalah jarak antar dokumen pertama dan kedua atau dengan kata lain disini kita bisa simpulkan bahwa tingkat kemiripan antara dokumen pertama dengan dokumen kedua adalah yang tertinggi diantara ketiga dokumen tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Word Filtering pada Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya yang akan kita pelajari adalah \"Stop Word Filtering\". Stop Word Filtering menyederhanakan representasi text dengan mengabaikan beberapa kata seperti determiners (the, a, an), auxiliary verbs (do, be, will), dan prepositions (on, in, at). \n",
    "\n",
    "Stop Word Filtering juga cukup umum ditemui dalam bidang NLP atau Natural Language Processing.\n",
    "\n",
    "Referensi: https://en.wikipedia.org/wiki/Stop_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk kasus kali ini kita akan kembali menggunakan corpus atau dataset yang sudah pernah kita gunakan sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the linux kernel.',\n",
       " 'Linux is one of the most prominent open-source software.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 05\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika kita lihat pada hasil output code 05, terdapat beberapa stop word yaitu \"the\", \"has\", \"been\", \"is\", dan \"of\" yang merupakan stop word yang akan kita abaikan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Word Filtering dengan CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disini kita kana memanfaatkan Stop Word Filtering untuk mengeluarkan stopword dari corpus yang kita miliki. Pada SkLearn, Stop Word Filtering ini juga dapat diterapkan dengan memanfatkan CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 2, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 06\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alur berpikir code 06 :\n",
    "- Pertama-tama kita akan mengimport terlebih dahulu modulnya dengan memanggil \"from sklearn.feature_extraction.text import CountVectorizer\".\n",
    "- Lalu selanjutnya kita akan membentuk terlebih dahulu objek dari class Count Vectorizer tersebut. Yang berbeda kali ini adalah sewaktu kita membentuk objek Count Vectorizer, kita juga akan menyertakan parameter \"stop_words\" yang kita beri nilai 'english' karena pada kasus kali ini untuk melakukan stop word filtering untuk bahasa inggris. Lalu selanjutnya objek Count Vectorizer yang terbentuk akan kita tampung kedalam variabel \"vectorized\".\n",
    "- Objek \"vectorized\" ini kemudian akan kita panggil untuk menerapkan \"fit_tranform\" terhadap corpus yang kita miliki dan hasil tranformnya kita akan konversikan ke dalam array dua dimensi. Oleh karenanya, kita akan memnaggil method \".todense()\" dan array dua dimensi yang terbentuk akan kita tampung kedalam variabel \"vectorized_X\".\n",
    "- Lalu kita tampilkan hasilnya.\n",
    "\n",
    "Berdasarkan hasil output code 06, angka-angka pada baris pertama, kedua, dan ketiga tersebut merepresentasikan kalimat pertama, kalimat kedua, dan kalimat ketiga. Jika kita perhatikan lagi pada hasil output code 06, ukurannya jauh lebih kecil bila dibandingkan sebelumnya.\n",
    "\n",
    "Untuk melihat representasinya dengan lebih rinci, maka kita bisa panggil method \"vectorizer.get_feature_names()\" (lihat pada code 07)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'distributions',\n",
       " 'include',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'software',\n",
       " 'source']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 07\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan hasil output code 07 merupakan kumpulan kata atau tokens yang sudah kita filtering stop wordsnya. Bisa kita lihat pada hasil code 07 tersebut bahwa kita sudah tidak lagi menyertakan stop words dalam kumpulan kata atau dalam token yang kita miliki. Alhasil, kita akan memperoleh suatu representasi dari suatu kalimat yang lebih sederhana.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk pembelajaran lebih lengkapnya, jangan lupa kunjungi channel youtube Indonesia Belajar pada link berikut : https://www.youtube.com/watch?v=U30sF4m0bd0 dan jangan lupa untuk like, comment, dan share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Banyak Belajar, Biar Bisa Bantu Banyak Orang\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by : Clarence Code Pianist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
